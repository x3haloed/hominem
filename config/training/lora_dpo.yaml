model:
  base_model_id: "Qwen/Qwen3-1.7B"
  # smaller model, but keep this modest on 16GB
  max_length: 1024
  padding_side: "left"
  load_in_4bit: false           # Set true only if you have bitsandbytes + CUDA
  torch_dtype: "bfloat16"       # Choices: auto | float16 | bfloat16
  gradient_checkpointing: true

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  bias: "none"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

training:
  # Preferences can be JSONL or Parquet file
  # For Parquet: export from database using scripts/export/export_preferences.py
  preferences_path: "data/preferences/preferences.jsonl"  # or "data/exports/preferences.parquet"
  output_dir: "artifacts/lora/qwen3.1-7b-v2"
  batch_size: 2         # Increased from 1 - more stable gradients
  # Improved hyperparameters based on training analysis
  num_epochs: 3         # Increased epochs for better convergence
  learning_rate: 5.0e-6 # Reduced 3x - was causing instability
  warmup_steps: 100     # Increased for more stable training start
  max_grad_norm: 0.3    # Reduced for more conservative updates
  logging_steps: 10     # Less frequent logging for cleaner output
  save_every_steps: 0   # 0 = only save at end
  beta: 0.05           # Reduced - was too high, causing instability
  seed: 42



