model:
  base_model_id: "Qwen/Qwen3-1.7B"
  # smaller model, but keep this modest on 16GB
  max_length: 1024
  padding_side: "left"
  load_in_4bit: false           # Set true only if you have bitsandbytes + CUDA
  torch_dtype: "bfloat16"       # Choices: auto | float16 | bfloat16
  gradient_checkpointing: true

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  bias: "none"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

training:
  preferences_path: "data/preferences/preferences.jsonl"
  output_dir: "artifacts/lora/qwen3.1-7b"
  batch_size: 1
  # On-device training: keep epochs modest and LR conservative.
  num_epochs: 2
  learning_rate: 1.5e-5
  warmup_steps: 50
  max_grad_norm: 0.5
  logging_steps: 5
  save_every_steps: 0   # 0 = only save at end
  beta: 0.1             # DPO-style temperature coefficient
  seed: 42



