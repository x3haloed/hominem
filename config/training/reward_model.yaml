model_id: "bert-base-uncased"
max_length: 512

# On M3 Pro with 16GB, this keeps memory usage modest while
# letting you iterate quickly as you expand reward_samples.
batch_size: 8
num_epochs: 4
learning_rate: 4.0e-5
weight_decay: 0.01

val_ratio: 0.12
seed: 42

# Enable label standardization (per-dimension z-score) during training.
standardize_labels: true

# Per-dimension loss weights (aligned with REWARD_MODEL_TARGETS order).
# Adjust to emphasize/de-emphasize specific dimensions.
per_dim_weights:
  - 1.0  # empathy
  - 1.0  # social_coherence
  - 1.0  # agency_support
  - 1.0  # epistemic_integrity
  - 1.0  # harm_avoidance
  - 1.0  # narrative_alignment
  - 1.0  # curiosity
  - 1.0  # reward_intensity
  - 1.0  # safety_score

# Linear LR warmup steps (integer). ~5% of total steps is a good start.
warmup_steps: 28

